{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00502454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "#from torchinfo import summary\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "#from torch.data import random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b236d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Convs_Unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69300e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f537af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Unet model\n",
      "Using device cuda\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = UNet(n_channels = 3 , n_classes = 1)\n",
    "print(\"Loading Unet model\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "model.to(device=device)\n",
    "model.load_state_dict(torch.load(\"../Tiles_512_Sigmod6CT_ep150.pth\", map_location=device))\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b185e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b5b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(torch.from_numpy(np.random.rand(1,3,300,300)).to(device = device, dtype = torch.float32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31b7e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "add4cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inc.double_conv.0.weight torch.Size([64, 3, 3, 3])\n",
      "inc.double_conv.0.bias torch.Size([64])\n",
      "inc.double_conv.1.weight torch.Size([64])\n",
      "inc.double_conv.1.bias torch.Size([64])\n",
      "inc.double_conv.1.running_mean torch.Size([64])\n",
      "inc.double_conv.1.running_var torch.Size([64])\n",
      "inc.double_conv.1.num_batches_tracked torch.Size([])\n",
      "inc.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "inc.double_conv.3.bias torch.Size([64])\n",
      "inc.double_conv.4.weight torch.Size([64])\n",
      "inc.double_conv.4.bias torch.Size([64])\n",
      "inc.double_conv.4.running_mean torch.Size([64])\n",
      "inc.double_conv.4.running_var torch.Size([64])\n",
      "inc.double_conv.4.num_batches_tracked torch.Size([])\n",
      "down1.maxpool_conv.1.double_conv.0.weight torch.Size([128, 64, 3, 3])\n",
      "down1.maxpool_conv.1.double_conv.0.bias torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.1.weight torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.1.bias torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.1.running_mean torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.1.running_var torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "down1.maxpool_conv.1.double_conv.3.weight torch.Size([128, 128, 3, 3])\n",
      "down1.maxpool_conv.1.double_conv.3.bias torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.4.weight torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.4.bias torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.4.running_mean torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.4.running_var torch.Size([128])\n",
      "down1.maxpool_conv.1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "down2.maxpool_conv.1.double_conv.0.weight torch.Size([256, 128, 3, 3])\n",
      "down2.maxpool_conv.1.double_conv.0.bias torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.1.weight torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.1.bias torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.1.running_mean torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.1.running_var torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "down2.maxpool_conv.1.double_conv.3.weight torch.Size([256, 256, 3, 3])\n",
      "down2.maxpool_conv.1.double_conv.3.bias torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.4.weight torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.4.bias torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.4.running_mean torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.4.running_var torch.Size([256])\n",
      "down2.maxpool_conv.1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "down3.maxpool_conv.1.double_conv.0.weight torch.Size([512, 256, 3, 3])\n",
      "down3.maxpool_conv.1.double_conv.0.bias torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.1.weight torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.1.bias torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.1.running_mean torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.1.running_var torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "down3.maxpool_conv.1.double_conv.3.weight torch.Size([512, 512, 3, 3])\n",
      "down3.maxpool_conv.1.double_conv.3.bias torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.4.weight torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.4.bias torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.4.running_mean torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.4.running_var torch.Size([512])\n",
      "down3.maxpool_conv.1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "down4.maxpool_conv.1.double_conv.0.weight torch.Size([512, 512, 3, 3])\n",
      "down4.maxpool_conv.1.double_conv.0.bias torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.1.weight torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.1.bias torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.1.running_mean torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.1.running_var torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "down4.maxpool_conv.1.double_conv.3.weight torch.Size([512, 512, 3, 3])\n",
      "down4.maxpool_conv.1.double_conv.3.bias torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.4.weight torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.4.bias torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.4.running_mean torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.4.running_var torch.Size([512])\n",
      "down4.maxpool_conv.1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "up1.conv.double_conv.0.weight torch.Size([512, 1024, 3, 3])\n",
      "up1.conv.double_conv.0.bias torch.Size([512])\n",
      "up1.conv.double_conv.1.weight torch.Size([512])\n",
      "up1.conv.double_conv.1.bias torch.Size([512])\n",
      "up1.conv.double_conv.1.running_mean torch.Size([512])\n",
      "up1.conv.double_conv.1.running_var torch.Size([512])\n",
      "up1.conv.double_conv.1.num_batches_tracked torch.Size([])\n",
      "up1.conv.double_conv.3.weight torch.Size([256, 512, 3, 3])\n",
      "up1.conv.double_conv.3.bias torch.Size([256])\n",
      "up1.conv.double_conv.4.weight torch.Size([256])\n",
      "up1.conv.double_conv.4.bias torch.Size([256])\n",
      "up1.conv.double_conv.4.running_mean torch.Size([256])\n",
      "up1.conv.double_conv.4.running_var torch.Size([256])\n",
      "up1.conv.double_conv.4.num_batches_tracked torch.Size([])\n",
      "up2.conv.double_conv.0.weight torch.Size([256, 512, 3, 3])\n",
      "up2.conv.double_conv.0.bias torch.Size([256])\n",
      "up2.conv.double_conv.1.weight torch.Size([256])\n",
      "up2.conv.double_conv.1.bias torch.Size([256])\n",
      "up2.conv.double_conv.1.running_mean torch.Size([256])\n",
      "up2.conv.double_conv.1.running_var torch.Size([256])\n",
      "up2.conv.double_conv.1.num_batches_tracked torch.Size([])\n",
      "up2.conv.double_conv.3.weight torch.Size([128, 256, 3, 3])\n",
      "up2.conv.double_conv.3.bias torch.Size([128])\n",
      "up2.conv.double_conv.4.weight torch.Size([128])\n",
      "up2.conv.double_conv.4.bias torch.Size([128])\n",
      "up2.conv.double_conv.4.running_mean torch.Size([128])\n",
      "up2.conv.double_conv.4.running_var torch.Size([128])\n",
      "up2.conv.double_conv.4.num_batches_tracked torch.Size([])\n",
      "up3.conv.double_conv.0.weight torch.Size([128, 256, 3, 3])\n",
      "up3.conv.double_conv.0.bias torch.Size([128])\n",
      "up3.conv.double_conv.1.weight torch.Size([128])\n",
      "up3.conv.double_conv.1.bias torch.Size([128])\n",
      "up3.conv.double_conv.1.running_mean torch.Size([128])\n",
      "up3.conv.double_conv.1.running_var torch.Size([128])\n",
      "up3.conv.double_conv.1.num_batches_tracked torch.Size([])\n",
      "up3.conv.double_conv.3.weight torch.Size([64, 128, 3, 3])\n",
      "up3.conv.double_conv.3.bias torch.Size([64])\n",
      "up3.conv.double_conv.4.weight torch.Size([64])\n",
      "up3.conv.double_conv.4.bias torch.Size([64])\n",
      "up3.conv.double_conv.4.running_mean torch.Size([64])\n",
      "up3.conv.double_conv.4.running_var torch.Size([64])\n",
      "up3.conv.double_conv.4.num_batches_tracked torch.Size([])\n",
      "up4.conv.double_conv.0.weight torch.Size([64, 128, 3, 3])\n",
      "up4.conv.double_conv.0.bias torch.Size([64])\n",
      "up4.conv.double_conv.1.weight torch.Size([64])\n",
      "up4.conv.double_conv.1.bias torch.Size([64])\n",
      "up4.conv.double_conv.1.running_mean torch.Size([64])\n",
      "up4.conv.double_conv.1.running_var torch.Size([64])\n",
      "up4.conv.double_conv.1.num_batches_tracked torch.Size([])\n",
      "up4.conv.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "up4.conv.double_conv.3.bias torch.Size([64])\n",
      "up4.conv.double_conv.4.weight torch.Size([64])\n",
      "up4.conv.double_conv.4.bias torch.Size([64])\n",
      "up4.conv.double_conv.4.running_mean torch.Size([64])\n",
      "up4.conv.double_conv.4.running_var torch.Size([64])\n",
      "up4.conv.double_conv.4.num_batches_tracked torch.Size([])\n",
      "outc.conv.weight torch.Size([1, 64, 1, 1])\n",
      "outc.conv.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for x in model.state_dict():\n",
    "    print(x , model.state_dict()[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b0c16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (inc): DoubleConv(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): DownConv(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): DownConv(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): DownConv(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): DownConv(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up1): UpConv(\n",
      "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2): UpConv(\n",
      "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3): UpConv(\n",
      "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4): UpConv(\n",
      "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc): OutConv(\n",
      "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f82ca084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "UNet                                          [1, 1, 50, 50]            --\n",
       "├─DoubleConv: 1-1                             [1, 64, 50, 50]           --\n",
       "│    └─Sequential: 2-1                        [1, 64, 50, 50]           --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 50, 50]           1,792\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 64, 50, 50]           128\n",
       "│    │    └─ReLU: 3-3                         [1, 64, 50, 50]           --\n",
       "│    │    └─Conv2d: 3-4                       [1, 64, 50, 50]           36,928\n",
       "│    │    └─BatchNorm2d: 3-5                  [1, 64, 50, 50]           128\n",
       "│    │    └─ReLU: 3-6                         [1, 64, 50, 50]           --\n",
       "├─DownConv: 1-2                               [1, 128, 25, 25]          --\n",
       "│    └─Sequential: 2-2                        [1, 128, 25, 25]          --\n",
       "│    │    └─MaxPool2d: 3-7                    [1, 64, 25, 25]           --\n",
       "│    │    └─DoubleConv: 3-8                   [1, 128, 25, 25]          221,952\n",
       "├─DownConv: 1-3                               [1, 256, 12, 12]          --\n",
       "│    └─Sequential: 2-3                        [1, 256, 12, 12]          --\n",
       "│    │    └─MaxPool2d: 3-9                    [1, 128, 12, 12]          --\n",
       "│    │    └─DoubleConv: 3-10                  [1, 256, 12, 12]          886,272\n",
       "├─DownConv: 1-4                               [1, 512, 6, 6]            --\n",
       "│    └─Sequential: 2-4                        [1, 512, 6, 6]            --\n",
       "│    │    └─MaxPool2d: 3-11                   [1, 256, 6, 6]            --\n",
       "│    │    └─DoubleConv: 3-12                  [1, 512, 6, 6]            3,542,016\n",
       "├─DownConv: 1-5                               [1, 512, 3, 3]            --\n",
       "│    └─Sequential: 2-5                        [1, 512, 3, 3]            --\n",
       "│    │    └─MaxPool2d: 3-13                   [1, 512, 3, 3]            --\n",
       "│    │    └─DoubleConv: 3-14                  [1, 512, 3, 3]            4,721,664\n",
       "├─UpConv: 1-6                                 [1, 256, 6, 6]            --\n",
       "│    └─Upsample: 2-6                          [1, 512, 6, 6]            --\n",
       "│    └─DoubleConv: 2-7                        [1, 256, 6, 6]            --\n",
       "│    │    └─Sequential: 3-15                  [1, 256, 6, 6]            5,900,544\n",
       "├─UpConv: 1-7                                 [1, 128, 12, 12]          --\n",
       "│    └─Upsample: 2-8                          [1, 256, 12, 12]          --\n",
       "│    └─DoubleConv: 2-9                        [1, 128, 12, 12]          --\n",
       "│    │    └─Sequential: 3-16                  [1, 128, 12, 12]          1,475,712\n",
       "├─UpConv: 1-8                                 [1, 64, 25, 25]           --\n",
       "│    └─Upsample: 2-10                         [1, 128, 24, 24]          --\n",
       "│    └─DoubleConv: 2-11                       [1, 64, 25, 25]           --\n",
       "│    │    └─Sequential: 3-17                  [1, 64, 25, 25]           369,216\n",
       "├─UpConv: 1-9                                 [1, 64, 50, 50]           --\n",
       "│    └─Upsample: 2-12                         [1, 64, 50, 50]           --\n",
       "│    └─DoubleConv: 2-13                       [1, 64, 50, 50]           --\n",
       "│    │    └─Sequential: 3-18                  [1, 64, 50, 50]           110,976\n",
       "├─OutConv: 1-10                               [1, 1, 50, 50]            --\n",
       "│    └─Conv2d: 2-14                           [1, 1, 50, 50]            65\n",
       "===============================================================================================\n",
       "Total params: 17,267,393\n",
       "Trainable params: 17,267,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.46\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 17.98\n",
       "Params size (MB): 69.07\n",
       "Estimated Total Size (MB): 87.08\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model ,input_size = (1 ,3 ,50 ,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887e1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int = int\n",
    "def expand_by_mirroring(image_train_raw,\n",
    "                        len_row, \n",
    "                        len_col,\n",
    "                        half_len): #half_len = 512//4\n",
    "\n",
    "    \"\"\" \n",
    "# For testing,\n",
    "    1. expand left and up side by GLOBAL['len_Unet_tile']/4.\n",
    "    2. expand right and button side by \"residue + GLOBAL['len_Unet_tile']/4\", where residue equals \"GLOBAL['len_Unet_tile'] - len_row%GLOBAL['len_Unet_tile']\"\n",
    "\n",
    "\"\"\"\n",
    "    len_img_tile = 4*half_len\n",
    "    if len_row%len_img_tile == 0:\n",
    "        row_expand = np.int(len_row + 0.5 * len_img_tile)\n",
    "        col_expand = np.int(len_col + 0.5 * len_img_tile)        \n",
    "    else:\n",
    "        row_expand = np.int(len_row + 1.5 * len_img_tile - len_row%len_img_tile)\n",
    "        col_expand = np.int(len_col + 1.5 * len_img_tile - len_col%len_img_tile)\n",
    "    \n",
    "    img_expand = np.uint8(np.zeros([row_expand, col_expand, 3]))\n",
    "\n",
    "    \n",
    "\n",
    "    if len_row%len_img_tile == 0:\n",
    "        img_expand[half_len:half_len + len_row, half_len : half_len + len_col, :] = image_train_raw  \n",
    "        img_expand[:half_len, :, :] = np.flipud(img_expand[half_len : 2 * half_len, :, :])\n",
    "        img_expand[half_len + len_row:row_expand, :, :] = np.flipud(img_expand[half_len + len_row - half_len:half_len + len_row, :, :])\n",
    "        img_expand[:, :half_len, :] = np.fliplr(img_expand[:, half_len :  2 * half_len, :])\n",
    "        img_expand[:, half_len + len_col:col_expand, :] = np.fliplr(img_expand[:, half_len + len_col - half_len:half_len + len_col, :])    \n",
    "         \n",
    "    else:\n",
    "        img_expand[half_len:half_len + len_row, half_len : half_len + len_col, :] = image_train_raw  \n",
    "        img_expand[:half_len, :, :] = np.flipud(img_expand[half_len : 2 * half_len, :, :])\n",
    "        img_expand[half_len + len_row:row_expand, :, :] = np.flipud(img_expand[half_len + len_row - (5 * half_len - len_row%len_img_tile):half_len + len_row, :, :])\n",
    "        img_expand[:, :half_len, :] = np.fliplr(img_expand[:, half_len :  2 * half_len, :])\n",
    "        img_expand[:, half_len + len_col:col_expand, :] = np.fliplr(img_expand[:, half_len + len_col - (5 * half_len - len_col%len_img_tile):half_len + len_col, :])    \n",
    "        \n",
    "    return img_expand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba59fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmt5_0325(img_expand,\n",
    "                aug_method):\n",
    "\n",
    "    aug_dict = {\"rt00\":rt00(img_expand),\n",
    "                \"rt90\":rt90(img_expand),\n",
    "                \"rt180\":rt180(img_expand),\n",
    "                \"fplr\":fplr(img_expand), \n",
    "                \"fpud\":fpud(img_expand)}\n",
    "    image = aug_dict[aug_method]    \n",
    "    \n",
    "    return image\n",
    "\n",
    "def rt00(img):\n",
    "    img_aug = img\n",
    "    return img_aug\n",
    "\n",
    "def rt90(img):\n",
    "    img_aug = np.rot90(img)\n",
    "    return img_aug\n",
    "\n",
    "def rt180(img):\n",
    "    img_aug = np.rot90(img, 2)\n",
    "    return img_aug\n",
    "\n",
    "def fplr(img):\n",
    "    img_aug = iaa.Fliplr(1.0)(image = img)\n",
    "    return img_aug\n",
    "\n",
    "def fpud(img):\n",
    "    img_aug = iaa.Flipud(1.0)(image = img)\n",
    "    return img_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c52486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 7500, 3)\n",
      "(4500, 7500, 3)\n",
      "(4500, 7500, 3)\n",
      "(5100, 8500, 3)\n",
      "(5100, 8500, 3)\n",
      "(5100, 8500, 3)\n",
      "(4500, 7500, 3)\n",
      "(5100, 8500, 3)\n"
     ]
    }
   ],
   "source": [
    "dirs1 = os.listdir(\"../Data/Cropped/Im_inp/\")\n",
    "dirs2 = os.listdir(\"../Data/Cropped/Im_out/\")\n",
    "aug_list = ['rt00', 'rt90', 'rt180', 'fplr', 'fpud']\n",
    "new_dir = \"../Data/processed/\"\n",
    "for i , dir_ in enumerate(dirs1):\n",
    "    new_img = Image.open(\"../Data/Cropped/Im_inp/\" + dir_)\n",
    "    new_img = np.array(new_img)[:,:,:3]\n",
    "    print(new_img.shape)\n",
    "    for aug in aug_list :\n",
    "        new_img1 = augmt5_0325(new_img , aug)\n",
    "        shape = np.array(new_img1).shape\n",
    "        new_new_img1 = expand_by_mirroring(new_img1 , shape[0] , shape[1] ,512//4) \n",
    "        new_new_img1 = Image.fromarray(new_new_img1)\n",
    "        new_new_img1.save(new_dir + \"Im_inp/\" + dir_.split('.')[0]+ \"_\" + aug + \".tif\")\n",
    "\n",
    "for i , dir_ in enumerate(dirs2):\n",
    "    new_img = Image.open(\"../Data/Cropped/Im_out/\" + dir_)\n",
    "    new_img = np.array(new_img)[:,:,:3]\n",
    "    print(new_img.shape)\n",
    "    for aug in aug_list :\n",
    "        new_img1 = augmt5_0325(new_img , aug)\n",
    "        shape = np.array(new_img1).shape\n",
    "        new_new_img1 = expand_by_mirroring(new_img1 , shape[0] , shape[1] , 512//4)\n",
    "        new_new_img1 = Image.fromarray(new_new_img1)\n",
    "        new_new_img1.save(new_dir + \"Im_out/\" + dir_.split('.')[0] + \"_\" + aug + \".tif\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9764beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_aug(img ,img_name, step_x , step_y ,  dim_x ,dim_y , save_loc = \"../Data/Full_Aug/Im_inp/\"):\n",
    "    c = 0\n",
    "    for i in range(0 ,img.shape[0] , step_x):\n",
    "        if i + dim_x > img.shape[0]:\n",
    "            break\n",
    "        for j in range(0 , img.shape[1] , step_y):\n",
    "            if j + dim_y > img.shape[1] :\n",
    "                break \n",
    "            new_img = img[i : i + dim_x , j : j + dim_y ,:].copy()\n",
    "            new_img = Image.fromarray(new_img)\n",
    "            new_img.save(save_loc + img_name + f\"_{i}_{j}\" + \".tif\")\n",
    "            c+=1\n",
    "    print(\"Done !!\")\n",
    "    print(f\"Created {c} augmented images of {img_name} of size : {(dim_x ,dim_y)} taking the step size : {(step_x ,step_y)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b65da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_0_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_0_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_0_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_0_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_0_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_1_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_1_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_1_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_1_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_1_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_2_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_2_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_2_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_2_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_2_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_0_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_0_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_0_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_0_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_0_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_1_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_1_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_1_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_1_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_1_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_2_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_2_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_2_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_2_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_2_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 3300 augmented images of MC16_S1_rt90 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_fplr of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_fpud of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_rt00 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_rt180 of size : (512, 512) taking the step size : (100, 100)\n",
      "Done !!\n",
      "Created 4165 augmented images of MC16_S2_rt90 of size : (512, 512) taking the step size : (100, 100)\n"
     ]
    }
   ],
   "source": [
    "dirs1 = os.listdir(\"../Data/processed/Im_inp/\")\n",
    "dirs2 = os.listdir(\"../Data/processed/Im_out/\")\n",
    "\n",
    "for i , name in enumerate(dirs1):\n",
    "    img_aug(np.array(Image.open(\"../Data/processed/Im_inp/\" + name)) , name.split(\".\")[0] , 100 , 100 ,512 ,512 ,\"../Data/Full_Aug/Im_inp/\" )\n",
    "for i , name in enumerate(dirs2):\n",
    "    img_aug(np.array(Image.open(\"../Data/processed/Im_out/\" + name)) , name.split(\".\")[0] , 100 , 100 ,512 ,512 , \"../Data/Full_Aug/Im_out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e109456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
